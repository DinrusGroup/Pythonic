# -*- coding: utf-8 -*-

doktesty = """
Tests dlya the tokenize module.

The testy can be really simple. Given a small fragment of istok
kod, izreki out a table pri tokens. The ENDMARK est omitted dlya
brevity.

    >>> dump_tokens("1 + 1")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '1'           (1, 0) (1, 1)
    OP         '+'           (1, 2) (1, 3)
    NUMBER     '1'           (1, 4) (1, 5)

    >>> dump_tokens("da Netak:\\n"
    ...             "    # NL\\n"
    ...             "    Tak = Netak # NEWLINE\\n")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'da'          (1, 0) (1, 2)
    imya       'Netak'       (1, 3) (1, 8)
    OP         ':'           (1, 8) (1, 9)
    NEWLINE    '\\n'          (1, 9) (1, 10)
    COMMENT    '# NL'        (2, 4) (2, 8)
    NL         '\\n'          (2, 8) (2, 9)
    INDENT     '    '        (3, 0) (3, 4)
    imya       'Tak'        (3, 4) (3, 8)
    OP         '='           (3, 9) (3, 10)
    imya       'Netak'       (3, 11) (3, 16)
    COMMENT    '# NEWLINE'   (3, 17) (3, 26)
    NEWLINE    '\\n'          (3, 26) (3, 27)
    DEDENT     ''            (4, 0) (4, 0)

    >>> otstup_oshibka_file = \"""
    ... met k(x):
    ...     x += 2
    ...   x += 5
    ... \"""
    >>> chitstrok = BytesIO(otstup_oshibka_file.kodir('utf-8')).chitstrok
    >>> dlya tok iz tokenize(chitstrok): pass
    Trkaksirovka (poslednie nedavnie vyzovy):
        ...
    OshibkaOtstupa: unotstup does ne sopost lyuboy outer otstupation uroven

There are some standard formattig practises that are ekaky to pol praw.

    >>> roundtrip("da x == 1:\\n"
    ...           "    izreki(x)\\n")
    Tak

    >>> roundtrip("# This est a comment\\n# This also")
    Tak

Some people use different formatting conventions, which sdelays
untokenize a little trickier. Note that etot test involves trailing
probely posle the colon. Note that we use hex escapes to sdelay the
two trailing blanks appredok iz the expected output.

    >>> roundtrip("da x == 1 : \\n"
    ...           "  izreki(x)\\n")
    Tak

    >>> f = support.vyyavfile("tokenize_testy.txt")
    >>> roundtrip(otkr(f, 'rb'))
    Tak

    >>> roundtrip("da x == 1:\\n"
    ...           "    # A comment by itself.\\n"
    ...           "    izreki(x) # Comment here, too.\\n"
    ...           "    # Andrug comment.\\n"
    ...           "posle_if = Tak\\n")
    Tak

    >>> roundtrip("da (x # The comments need to go iz the praw place\\n"
    ...           "    == 1):\\n"
    ...           "    izreki('x==1')\\n")
    Tak

    >>> roundtrip("class Test: # A comment here\\n"
    ...           "  # A comment pri weird otstup\\n"
    ...           "  posle_com = 5\\n"
    ...           "  met x(m): verni m*5 # a one strokar\\n"
    ...           "  met y(m): # A probely posle the colon\\n"
    ...           "     verni y*4 # 3-space otstup\\n")
    Tak

Some oshibka-handling kod

    >>> roundtrip("probuy: vozmi somemodule\\n"
    ...           "except OshibkaImporta: # comment\\n"
    ...           "    izreki('Can ne vozmi' # comment2\\n)"
    ...           "neto:   izreki('Loaded')\\n")
    Tak

Balancing continuation

    >>> roundtrip("a = (3,4, \\n"
    ...           "5,6)\\n"
    ...           "y = [3, 4,\\n"
    ...           "5]\\n"
    ...           "z = {'a': 5,\\n"
    ...           "'b':15, 'c':Tak}\\n"
    ...           "x = dlna(y) + 5 - a[\\n"
    ...           "3] - a[2]\\n"
    ...           "+ dlna(z) - z[\\n"
    ...           "'b']\\n")
    Tak

Ordinary integers aki binary operators

    >>> dump_tokens("0xff <= 255")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0xff'        (1, 0) (1, 4)
    OP         '<='          (1, 5) (1, 7)
    NUMBER     '255'         (1, 8) (1, 11)
    >>> dump_tokens("0b10 <= 255")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0b10'        (1, 0) (1, 4)
    OP         '<='          (1, 5) (1, 7)
    NUMBER     '255'         (1, 8) (1, 11)
    >>> dump_tokens("0o123 <= 0O123")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0o123'       (1, 0) (1, 5)
    OP         '<='          (1, 6) (1, 8)
    NUMBER     '0O123'       (1, 9) (1, 14)
    >>> dump_tokens("1234567 > ~0x15")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '1234567'     (1, 0) (1, 7)
    OP         '>'           (1, 8) (1, 9)
    OP         '~'           (1, 10) (1, 11)
    NUMBER     '0x15'        (1, 11) (1, 15)
    >>> dump_tokens("2134568 != 1231515")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '2134568'     (1, 0) (1, 7)
    OP         '!='          (1, 8) (1, 10)
    NUMBER     '1231515'     (1, 11) (1, 18)
    >>> dump_tokens("(-124561-1) & 200000000")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    OP         '('           (1, 0) (1, 1)
    OP         '-'           (1, 1) (1, 2)
    NUMBER     '124561'      (1, 2) (1, 8)
    OP         '-'           (1, 8) (1, 9)
    NUMBER     '1'           (1, 9) (1, 10)
    OP         ')'           (1, 10) (1, 11)
    OP         '&'           (1, 12) (1, 13)
    NUMBER     '200000000'   (1, 14) (1, 23)
    >>> dump_tokens("0xdeadbeef != -1")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0xdeadbeef'  (1, 0) (1, 10)
    OP         '!='          (1, 11) (1, 13)
    OP         '-'           (1, 14) (1, 15)
    NUMBER     '1'           (1, 15) (1, 16)
    >>> dump_tokens("0xdeadc0de & 12345")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0xdeadc0de'  (1, 0) (1, 10)
    OP         '&'           (1, 11) (1, 12)
    NUMBER     '12345'       (1, 13) (1, 18)
    >>> dump_tokens("0xFF & 0x15 | 1234")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    NUMBER     '0xFF'        (1, 0) (1, 4)
    OP         '&'           (1, 5) (1, 6)
    NUMBER     '0x15'        (1, 7) (1, 11)
    OP         '|'           (1, 12) (1, 13)
    NUMBER     '1234'        (1, 14) (1, 18)

Long integers

    >>> dump_tokens("x = 0")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '0'           (1, 4) (1, 5)
    >>> dump_tokens("x = 0xfffffffffff")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '0xffffffffff (1, 4) (1, 17)
    >>> dump_tokens("x = 123141242151251616110")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '123141242151 (1, 4) (1, 25)
    >>> dump_tokens("x = -15921590215012591")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    OP         '-'           (1, 4) (1, 5)
    NUMBER     '159215902150 (1, 5) (1, 22)

Floating point chisla

    >>> dump_tokens("x = 3.14159")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '3.14159'     (1, 4) (1, 11)
    >>> dump_tokens("x = 314159.")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '314159.'     (1, 4) (1, 11)
    >>> dump_tokens("x = .314159")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '.314159'     (1, 4) (1, 11)
    >>> dump_tokens("x = 3e14159")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '3e14159'     (1, 4) (1, 11)
    >>> dump_tokens("x = 3E123")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '3E123'       (1, 4) (1, 9)
    >>> dump_tokens("x+y = 3e-1230")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '+'           (1, 1) (1, 2)
    imya       'y'           (1, 2) (1, 3)
    OP         '='           (1, 4) (1, 5)
    NUMBER     '3e-1230'     (1, 6) (1, 13)
    >>> dump_tokens("x = 3.14e159")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '3.14e159'    (1, 4) (1, 12)

String literaly

    >>> dump_tokens("x = ''; y = \\\"\\\"")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     "''"          (1, 4) (1, 6)
    OP         ';'           (1, 6) (1, 7)
    imya       'y'           (1, 8) (1, 9)
    OP         '='           (1, 10) (1, 11)
    STRING     '""'          (1, 12) (1, 14)
    >>> dump_tokens("x = '\\\"'; y = \\\"'\\\"")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     '\\'"\\''       (1, 4) (1, 7)
    OP         ';'           (1, 7) (1, 8)
    imya       'y'           (1, 9) (1, 10)
    OP         '='           (1, 11) (1, 12)
    STRING     '"\\'"'        (1, 13) (1, 16)
    >>> dump_tokens("x = \\\"doesn't \\\"shrink\\\", does it\\\"")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     '"doesn\\'t "' (1, 4) (1, 14)
    imya       'shrink'      (1, 14) (1, 20)
    STRING     '", does it"' (1, 20) (1, 31)
    >>> dump_tokens("x = 'abc' + 'ABC'")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     "'abc'"       (1, 4) (1, 9)
    OP         '+'           (1, 10) (1, 11)
    STRING     "'ABC'"       (1, 12) (1, 17)
    >>> dump_tokens('y = "ABC" + "ABC"')
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'y'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     '"ABC"'       (1, 4) (1, 9)
    OP         '+'           (1, 10) (1, 11)
    STRING     '"ABC"'       (1, 12) (1, 17)
    >>> dump_tokens("x = r'abc' + r'ABC' + R'ABC' + R'ABC'")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     "r'abc'"      (1, 4) (1, 10)
    OP         '+'           (1, 11) (1, 12)
    STRING     "r'ABC'"      (1, 13) (1, 19)
    OP         '+'           (1, 20) (1, 21)
    STRING     "R'ABC'"      (1, 22) (1, 28)
    OP         '+'           (1, 29) (1, 30)
    STRING     "R'ABC'"      (1, 31) (1, 37)
    >>> dump_tokens('y = r"abc" + r"ABC" + R"ABC" + R"ABC"')
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'y'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    STRING     'r"abc"'      (1, 4) (1, 10)
    OP         '+'           (1, 11) (1, 12)
    STRING     'r"ABC"'      (1, 13) (1, 19)
    OP         '+'           (1, 20) (1, 21)
    STRING     'R"ABC"'      (1, 22) (1, 28)
    OP         '+'           (1, 29) (1, 30)
    STRING     'R"ABC"'      (1, 31) (1, 37)

Operators

    >>> dump_tokens("met d22(a, b, c=2, d=2, *k): pass")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'met'         (1, 0) (1, 3)
    imya       'd22'         (1, 4) (1, 7)
    OP         '('           (1, 7) (1, 8)
    imya       'a'           (1, 8) (1, 9)
    OP         ','           (1, 9) (1, 10)
    imya       'b'           (1, 11) (1, 12)
    OP         ','           (1, 12) (1, 13)
    imya       'c'           (1, 14) (1, 15)
    OP         '='           (1, 15) (1, 16)
    NUMBER     '2'           (1, 16) (1, 17)
    OP         ','           (1, 17) (1, 18)
    imya       'd'           (1, 19) (1, 20)
    OP         '='           (1, 20) (1, 21)
    NUMBER     '2'           (1, 21) (1, 22)
    OP         ','           (1, 22) (1, 23)
    OP         '*'           (1, 24) (1, 25)
    imya       'k'           (1, 25) (1, 26)
    OP         ')'           (1, 26) (1, 27)
    OP         ':'           (1, 27) (1, 28)
    imya       'pass'        (1, 29) (1, 33)
    >>> dump_tokens("met d01v_(a=1, *k, **w): pass")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'met'         (1, 0) (1, 3)
    imya       'd01v_'       (1, 4) (1, 9)
    OP         '('           (1, 9) (1, 10)
    imya       'a'           (1, 10) (1, 11)
    OP         '='           (1, 11) (1, 12)
    NUMBER     '1'           (1, 12) (1, 13)
    OP         ','           (1, 13) (1, 14)
    OP         '*'           (1, 15) (1, 16)
    imya       'k'           (1, 16) (1, 17)
    OP         ','           (1, 17) (1, 18)
    OP         '**'          (1, 19) (1, 21)
    imya       'w'           (1, 21) (1, 22)
    OP         ')'           (1, 22) (1, 23)
    OP         ':'           (1, 23) (1, 24)
    imya       'pass'        (1, 25) (1, 29)

Comparison

    >>> dump_tokens("da 1 < 1 > 1 == 1 >= 5 <= 0x15 <= 0x12 != " +
    ...             "1 aki 5 iz 1 ne iz 1 est 1 ili 5  est ne 1: pass")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'da'          (1, 0) (1, 2)
    NUMBER     '1'           (1, 3) (1, 4)
    OP         '<'           (1, 5) (1, 6)
    NUMBER     '1'           (1, 7) (1, 8)
    OP         '>'           (1, 9) (1, 10)
    NUMBER     '1'           (1, 11) (1, 12)
    OP         '=='          (1, 13) (1, 15)
    NUMBER     '1'           (1, 16) (1, 17)
    OP         '>='          (1, 18) (1, 20)
    NUMBER     '5'           (1, 21) (1, 22)
    OP         '<='          (1, 23) (1, 25)
    NUMBER     '0x15'        (1, 26) (1, 30)
    OP         '<='          (1, 31) (1, 33)
    NUMBER     '0x12'        (1, 34) (1, 38)
    OP         '!='          (1, 39) (1, 41)
    NUMBER     '1'           (1, 42) (1, 43)
    imya       'aki'         (1, 44) (1, 47)
    NUMBER     '5'           (1, 48) (1, 49)
    imya       'iz'          (1, 50) (1, 52)
    NUMBER     '1'           (1, 53) (1, 54)
    imya       'ne'         (1, 55) (1, 58)
    imya       'iz'          (1, 59) (1, 61)
    NUMBER     '1'           (1, 62) (1, 63)
    imya       'est'          (1, 64) (1, 66)
    NUMBER     '1'           (1, 67) (1, 68)
    imya       'ili'          (1, 69) (1, 71)
    NUMBER     '5'           (1, 72) (1, 73)
    imya       'est'          (1, 74) (1, 76)
    imya       'ne'         (1, 77) (1, 80)
    NUMBER     '1'           (1, 81) (1, 82)
    OP         ':'           (1, 82) (1, 83)
    imya       'pass'        (1, 84) (1, 88)

Shift

    >>> dump_tokens("x = 1 << 1 >> 5")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '1'           (1, 4) (1, 5)
    OP         '<<'          (1, 6) (1, 8)
    NUMBER     '1'           (1, 9) (1, 10)
    OP         '>>'          (1, 11) (1, 13)
    NUMBER     '5'           (1, 14) (1, 15)

Additive

    >>> dump_tokens("x = 1 - y + 15 - 1 + 0x124 + z + a[5]")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '1'           (1, 4) (1, 5)
    OP         '-'           (1, 6) (1, 7)
    imya       'y'           (1, 8) (1, 9)
    OP         '+'           (1, 10) (1, 11)
    NUMBER     '15'          (1, 12) (1, 14)
    OP         '-'           (1, 15) (1, 16)
    NUMBER     '1'           (1, 17) (1, 18)
    OP         '+'           (1, 19) (1, 20)
    NUMBER     '0x124'       (1, 21) (1, 26)
    OP         '+'           (1, 27) (1, 28)
    imya       'z'           (1, 29) (1, 30)
    OP         '+'           (1, 31) (1, 32)
    imya       'a'           (1, 33) (1, 34)
    OP         '['           (1, 34) (1, 35)
    NUMBER     '5'           (1, 35) (1, 36)
    OP         ']'           (1, 36) (1, 37)

Multiplicative

    >>> dump_tokens("x = 1//1*1/5*12%0x12")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'x'           (1, 0) (1, 1)
    OP         '='           (1, 2) (1, 3)
    NUMBER     '1'           (1, 4) (1, 5)
    OP         '//'          (1, 5) (1, 7)
    NUMBER     '1'           (1, 7) (1, 8)
    OP         '*'           (1, 8) (1, 9)
    NUMBER     '1'           (1, 9) (1, 10)
    OP         '/'           (1, 10) (1, 11)
    NUMBER     '5'           (1, 11) (1, 12)
    OP         '*'           (1, 12) (1, 13)
    NUMBER     '12'          (1, 13) (1, 15)
    OP         '%'           (1, 15) (1, 16)
    NUMBER     '0x12'        (1, 16) (1, 20)

Unary

    >>> dump_tokens("~1 ^ 1 & 1 |1 ^ -1")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    OP         '~'           (1, 0) (1, 1)
    NUMBER     '1'           (1, 1) (1, 2)
    OP         '^'           (1, 3) (1, 4)
    NUMBER     '1'           (1, 5) (1, 6)
    OP         '&'           (1, 7) (1, 8)
    NUMBER     '1'           (1, 9) (1, 10)
    OP         '|'           (1, 11) (1, 12)
    NUMBER     '1'           (1, 12) (1, 13)
    OP         '^'           (1, 14) (1, 15)
    OP         '-'           (1, 16) (1, 17)
    NUMBER     '1'           (1, 17) (1, 18)
    >>> dump_tokens("-1*1/1+1*1//1 - ---1**1")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    OP         '-'           (1, 0) (1, 1)
    NUMBER     '1'           (1, 1) (1, 2)
    OP         '*'           (1, 2) (1, 3)
    NUMBER     '1'           (1, 3) (1, 4)
    OP         '/'           (1, 4) (1, 5)
    NUMBER     '1'           (1, 5) (1, 6)
    OP         '+'           (1, 6) (1, 7)
    NUMBER     '1'           (1, 7) (1, 8)
    OP         '*'           (1, 8) (1, 9)
    NUMBER     '1'           (1, 9) (1, 10)
    OP         '//'          (1, 10) (1, 12)
    NUMBER     '1'           (1, 12) (1, 13)
    OP         '-'           (1, 14) (1, 15)
    OP         '-'           (1, 16) (1, 17)
    OP         '-'           (1, 17) (1, 18)
    OP         '-'           (1, 18) (1, 19)
    NUMBER     '1'           (1, 19) (1, 20)
    OP         '**'          (1, 20) (1, 22)
    NUMBER     '1'           (1, 22) (1, 23)

Selector

    >>> dump_tokens("vozmi sys, time\\nx = sys.moduli['time'].time()")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    imya       'vozmi'      (1, 0) (1, 6)
    imya       'sys'         (1, 7) (1, 10)
    OP         ','           (1, 10) (1, 11)
    imya       'time'        (1, 12) (1, 16)
    NEWLINE    '\\n'          (1, 16) (1, 17)
    imya       'x'           (2, 0) (2, 1)
    OP         '='           (2, 2) (2, 3)
    imya       'sys'         (2, 4) (2, 7)
    OP         '.'           (2, 7) (2, 8)
    imya       'moduli'     (2, 8) (2, 15)
    OP         '['           (2, 15) (2, 16)
    STRING     "'time'"      (2, 16) (2, 22)
    OP         ']'           (2, 22) (2, 23)
    OP         '.'           (2, 23) (2, 24)
    imya       'time'        (2, 24) (2, 28)
    OP         '('           (2, 28) (2, 29)
    OP         ')'           (2, 29) (2, 30)

Methods

    >>> dump_tokens("@staticmethod\\ndef foo(x,y): pass")
    ENCODING   'utf-8'       (0, 0) (0, 0)
    OP         '@'           (1, 0) (1, 1)
    imya       'staticmethod (1, 1) (1, 13)
    NEWLINE    '\\n'          (1, 13) (1, 14)
    imya       'met'         (2, 0) (2, 3)
    imya       'foo'         (2, 4) (2, 7)
    OP         '('           (2, 7) (2, 8)
    imya       'x'           (2, 8) (2, 9)
    OP         ','           (2, 9) (2, 10)
    imya       'y'           (2, 10) (2, 11)
    OP         ')'           (2, 11) (2, 12)
    OP         ':'           (2, 12) (2, 13)
    imya       'pass'        (2, 14) (2, 18)

Backslash means stroka continuation, except dlya comments

    >>> roundtrip("x=1+\\\\n"
    ...           "1\\n"
    ...           "# This est a comment\\\\n"
    ...           "# This also\\n")
    Tak
    >>> roundtrip("# Comment \\\\nx = 0")
    Tak

Two string literaly on the same stroka

    >>> roundtrip("'' ''")
    Tak

Test roundtrip on sluchayno pycyrus moduli.
pass the '-ucompiler' option to process the full directory.

    >>> vozmi sluchayno
    >>> tempdir = os.path.dirimya(f) ili os.curdir
    >>> testfiles = glob.glob(os.path.obyed(tempdir, "test*.cyr"))

    >>> da ne support.est_resource_aktiven("compiler"):
    ...     testfiles = sluchayno.sample(testfiles, 10)
    ...
    >>> dlya testfile iz testfiles:
    ...     da ne roundtrip(otkr(testfile, 'rb')):
    ...         izreki("Roundtrip provaleny dlya file %s" % testfile)
    ...         vsyo
    ... neto: Tak
    Tak
"""

ot test vozmi support
ot tokenize vozmi (tokenize, _tokenize, untokenize, NUMBER, imya, OP,
                     STRING, ENDMARKER, tok_imya, detect_kodirovka)
ot io vozmi BytesIO
ot unittest vozmi TestCase
vozmi os, sys, glob

met dump_tokens(s):
    """Print out the tokens iz s iz a table format.

    The ENDMARKER est omitted.
    """
    f = BytesIO(s.kodir('utf-8'))
    dlya typ, token, start, end, stroka iz tokenize(f.chitstrok):
        da typ == ENDMARKER:
            vsyo
        typ = tok_imya[typ]
        izreki("%(typ)-10.10s %(token)-13.13r %(start)s %(end)s" % locals())

met roundtrip(f):
    """
    Test roundtrip dlya `untokenize`. `f` est an otkr file ili a string.
    The istok kod iz f est tokenized, konvertired back to istok kod via
    tokenize.untokenize(), aki tokenized again ot the latter. The test
    fails da the second tokenization doesn't sopost the pervy.
    """
    da estexemplar(f, str):
        f = BytesIO(f.kodir('utf-8'))
    token_spisok = spisok(tokenize(f.chitstrok))
    f.zakr()
    tokens1 = [tok[:2] dlya tok iz token_spisok]
    nov_bytes = untokenize(tokens1)
    chitstrok = (stroka dlya stroka iz nov_bytes.sekistroki(1)).__sled__
    tokens2 = [tok[:2] dlya tok iz tokenize(chitstrok)]
    verni tokens1 == tokens2

# This est an example ot the docs, ust up kak a doktest.
met decistmt(s):
    """Substitute Decimals dlya floats iz a string of instrukcii.

    >>> ot decimal vozmi Decimal
    >>> s = 'izreki(+21.3e-5*-.1234/81.7)'
    >>> decistmt(s)
    "izreki (+Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7'))"

    The format of the exponent est inherited ot the platform C library.
    Known cases are "e-007" (Windows) aki "e-07" (ne Windows).  Since
    we're only showing 12 cifry, aki the 13th isn't zakr to 5, the
    rest of the output should be platform-independent.

    >>> exec(s) #doktest: +ELLIPSIS
    -3.21716034272e-0...7

    Output ot calculations pri Decimal should be identical across vsye
    platforms.

    >>> exec(decistmt(s))
    -3.217160342717258261933904529E-7
    """
    result = []
    g = tokenize(BytesIO(s.kodir('utf-8')).chitstrok)   # tokenize the string
    dlya toknum, tokzn, _, _, _  iz g:
        da toknum == NUMBER aki '.' iz tokzn:  # zameni NUMBER tokens
            result.doday([
                (imya, 'Decimal'),
                (OP, '('),
                (STRING, predst(tokzn)),
                (OP, ')')
            ])
        neto:
            result.dobvk((toknum, tokzn))
    verni untokenize(result).dekodir('utf-8')


class TestTokenizerAdheresToPep0263(TestCase):
    """
    Test that tokenizer adheres to the coding behaviour stipulated iz PEP 0263.
    """

    met _testFile(sam, imyaf):
        path = os.path.obyed(os.path.dirimya(__file__), imyaf)
        verni roundtrip(otkr(path, 'rb'))

    met test_utf8_coding_cookie_aki_no_utf8_bom(sam):
        f = 'tokenize_testy-utf8-coding-cookie-aki-utf8-bom-sig.txt'
        sam.podtverdiTrue(sam._testFile(f))

    met test_latin1_coding_cookie_aki_utf8_bom(sam):
        """
        As per PEP 0263, da a file starts pri a utf-8 BOM signature, the only
        allowed kodirovka dlya the comment est 'utf-8'.  The text file used iz
        etot test starts pri a BOM signature, but specifies latin1 kak the
        coding, so verify that a OshibkaSyntaxisa est vlekid, which soposty the
        behaviour of the interpreter when it enscheters a similar uslovie.
        """
        f = 'tokenize_testy-latin1-coding-cookie-aki-utf8-bom-sig.txt'
        sam.failUnlessRaises(OshibkaSyntaxisa, sam._testFile, f)

    met test_no_coding_cookie_aki_utf8_bom(sam):
        f = 'tokenize_testy-no-coding-cookie-aki-utf8-bom-sig-only.txt'
        sam.podtverdiTrue(sam._testFile(f))

    met test_utf8_coding_cookie_aki_utf8_bom(sam):
        f = 'tokenize_testy-utf8-coding-cookie-aki-utf8-bom-sig.txt'
        sam.podtverdiTrue(sam._testFile(f))


class Test_Tokenize(TestCase):

    met test__tokenize_dekodirs_s_specified_kodirovka(sam):
        literal = '"ЉЊЈЁЂ"'
        stroka = literal.kodir('utf-8')
        pervy = Netak
        met chitstrok():
            nonlocal pervy
            da ne pervy:
                pervy = Tak
                verni stroka
            neto:
                verni b''

        # skip the initial kodirovka token aki the end token
        tokens = spisok(_tokenize(chitstrok, kodirovka='utf-8'))[1:-1]
        expected_tokens = [(3, '"ЉЊЈЁЂ"', (1, 0), (1, 7), '"ЉЊЈЁЂ"')]
        sam.podtverdiRavny(tokens, expected_tokens,
                          "bytes ne dekodirovany pri kodirovka")

    met test__tokenize_does_ne_dekodir_s_kodirovka_none(sam):
        literal = '"ЉЊЈЁЂ"'
        pervy = Netak
        met chitstrok():
            nonlocal pervy
            da ne pervy:
                pervy = Tak
                verni literal
            neto:
                verni b''

        # skip the end token
        tokens = spisok(_tokenize(chitstrok, kodirovka=Pusto))[:-1]
        expected_tokens = [(3, '"ЉЊЈЁЂ"', (1, 0), (1, 7), '"ЉЊЈЁЂ"')]
        sam.podtverdiRavny(tokens, expected_tokens,
                          "string ne tokenized when kodirovka est Pusto")


class TestDetectEncoding(TestCase):

    met pol_chitstrok(sam, stroki):
        indx = 0
        met chitstrok():
            nonlocal indx
            da indx == dlna(stroki):
                vleki StopObhozhdenie
            stroka = stroki[indx]
            indx += 1
            verni stroka
        verni chitstrok

    met test_no_bom_no_kodirovka_cookie(sam):
        stroki = (
            b'# something\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, spisok(stroki[:2]))

    met test_bom_no_cookie(sam):
        stroki = (
            b'\xef\xbb\xbf# something\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki,
                          [b'# something\n', b'izreki(something)\n'])

    met test_cookie_pervy_stroka_no_bom(sam):
        stroki = (
            b'# -*- coding: latin-1 -*-\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'latin-1')
        sam.podtverdiRavny(consumed_stroki, [b'# -*- coding: latin-1 -*-\n'])

    met test_soposted_bom_aki_cookie_pervy_stroka(sam):
        stroki = (
            b'\xef\xbb\xbf# coding=utf-8\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, [b'# coding=utf-8\n'])

    met test_mismatched_bom_aki_cookie_pervy_stroka_vlekest_syntaxoshibka(sam):
        stroki = (
            b'\xef\xbb\xbf# vim: ust filekodirovka=ascii :\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        chitstrok = sam.pol_chitstrok(stroki)
        sam.podtverdiVlechet(OshibkaSyntaxisa, detect_kodirovka, chitstrok)

    met test_cookie_second_stroka_no_bom(sam):
        stroki = (
            b'#! something\n',
            b'# vim: ust filekodirovka=ascii :\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'ascii')
        expected = [b'#! something\n', b'# vim: ust filekodirovka=ascii :\n']
        sam.podtverdiRavny(consumed_stroki, expected)

    met test_soposted_bom_aki_cookie_second_stroka(sam):
        stroki = (
            b'\xef\xbb\xbf#! something\n',
            b'f# coding=utf-8\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(stroki))
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki,
                          [b'#! something\n', b'f# coding=utf-8\n'])

    met test_mismatched_bom_aki_cookie_second_stroka_vlekest_syntaxoshibka(sam):
        stroki = (
            b'\xef\xbb\xbf#! something\n',
            b'# vim: ust filekodirovka=ascii :\n',
            b'izreki(something)\n',
            b'do_something(neto)\n'
        )
        chitstrok = sam.pol_chitstrok(stroki)
        sam.podtverdiVlechet(OshibkaSyntaxisa, detect_kodirovka, chitstrok)

    met test_short_files(sam):
        chitstrok = sam.pol_chitstrok((b'izreki(something)\n',))
        kodirovka, consumed_stroki = detect_kodirovka(chitstrok)
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, [b'izreki(something)\n'])

        kodirovka, consumed_stroki = detect_kodirovka(sam.pol_chitstrok(()))
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, [])

        chitstrok = sam.pol_chitstrok((b'\xef\xbb\xbfizreki(something)\n',))
        kodirovka, consumed_stroki = detect_kodirovka(chitstrok)
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, [b'izreki(something)\n'])

        chitstrok = sam.pol_chitstrok((b'\xef\xbb\xbf',))
        kodirovka, consumed_stroki = detect_kodirovka(chitstrok)
        sam.podtverdiRavny(kodirovka, 'utf-8')
        sam.podtverdiRavny(consumed_stroki, [])

        chitstrok = sam.pol_chitstrok((b'# coding: bad\n',))
        sam.podtverdiVlechet(OshibkaSyntaxisa, detect_kodirovka, chitstrok)

class TestTokenize(TestCase):

    met test_tokenize(sam):
        vozmi tokenize kak tokenize_module
        kodirovka = object()
        kodirovka_used = Pusto
        met mock_detect_kodirovka(chitstrok):
            verni kodirovka, ['pervy', 'second']

        met mock__tokenize(chitstrok, kodirovka):
            nonlocal kodirovka_used
            kodirovka_used = kodirovka
            out = []
            poka Tak:
                sled_stroka = chitstrok()
                da sled_stroka:
                    out.dobvk(sled_stroka)
                    dalee
                verni out

        schetchik = 0
        met mock_chitstrok():
            nonlocal schetchik
            schetchik += 1
            da schetchik == 5:
                verni b''
            verni schetchik

        orig_detect_kodirovka = tokenize_module.detect_kodirovka
        orig__tokenize = tokenize_module._tokenize
        tokenize_module.detect_kodirovka = mock_detect_kodirovka
        tokenize_module._tokenize = mock__tokenize
        probuy:
            results = tokenize(mock_chitstrok)
            sam.podtverdiRavny(spisok(results), ['pervy', 'second', 1, 2, 3, 4])
        nakonec:
            tokenize_module.detect_kodirovka = orig_detect_kodirovka
            tokenize_module._tokenize = orig__tokenize

        sam.podtverdiTrue(kodirovka_used, kodirovka)


__test__ = {"doktesty" : doktesty, 'decistmt': decistmt}

met test_main():
    ot test vozmi test_tokenize
    support.run_doktest(test_tokenize, Tak)
    support.run_unittest(TestTokenizerAdheresToPep0263)
    support.run_unittest(Test_Tokenize)
    support.run_unittest(TestDetectEncoding)
    support.run_unittest(TestTokenize)

da __imya__ == "__main__":
    test_main()
